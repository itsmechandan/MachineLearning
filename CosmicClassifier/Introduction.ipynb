{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de36630",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The dataset contains Monte Carlo–simulated events from an atmospheric Cherenkov telescope, representing light patterns produced by high-energy particles interacting in the Earth’s atmosphere. Each event is described by 10 numerical features (Hillas parameters) that characterize the shape and intensity of the recorded shower image.\n",
    "\n",
    "The goal is a binary classification task — to distinguish gamma-ray events (signal) from hadronic cosmic-ray events (background) based on these geometric and brightness features.\n",
    "\n",
    "Data set Link: https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c08b4",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca510e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.11.7' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ee35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/venkatchandan/Desktop/ML_Projects/CosmicClassifier/magic+gamma+telescope/magic04.data')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc56f1",
   "metadata": {},
   "source": [
    "Looks Like the Dataset has no column headers and we have to manually enter the column headers from the Dataset Link. There are 11 column headers namely:\n",
    "\n",
    "| Variable Name | Role    | Type       | Description                                                      | Units | Missing Values |\n",
    "| ------------- | ------- | ---------- | ---------------------------------------------------------------- | ----- | -------------- |\n",
    "| fLength       | Feature | Continuous | Major axis of ellipse                                            | mm    | No             |\n",
    "| fWidth        | Feature | Continuous | Minor axis of ellipse                                            | mm    | No             |\n",
    "| fSize         | Feature | Continuous | 10-log of sum of content of all pixels                           | #phot | No             |\n",
    "| fConc         | Feature | Continuous | Ratio of sum of two highest pixels over fSize                    | —     | No             |\n",
    "| fConc1        | Feature | Continuous | Ratio of highest pixel over fSize                                | —     | No             |\n",
    "| fAsym         | Feature | Continuous | Distance from highest pixel to center, projected onto major axis | —     | No             |\n",
    "| fM3Long       | Feature | Continuous | 3rd root of third moment along major axis                        | mm    | No             |\n",
    "| fM3Trans      | Feature | Continuous | 3rd root of third moment along minor axis                        | mm    | No             |\n",
    "| fAlpha        | Feature | Continuous | Angle of major axis with vector to origin                        | deg   | No             |\n",
    "| fDist         | Feature | Continuous | Distance from origin to center of ellipse                        | mm    | No             |\n",
    "| class         | Target  | Binary     | gamma (signal), hadron (background)                              | -     | No             | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef0195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fLength','fWidth','fSize','fConc','fConc1','fAsym','fM3Long','fM3Trans','fAlpha','fDist','class']\n",
    "df1 = pd.read_csv('/Users/venkatchandan/Desktop/ML_Projects/CosmicClassifier/magic+gamma+telescope/magic04.data',names = cols)\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4867e67",
   "metadata": {},
   "source": [
    "The class column has two values namely g and h. We convert them to 1's and 0's as our computer cant understand Language.\n",
    "We can convert that into 1's and 0's using various methods\n",
    "```bash \n",
    "1. data['class'] = data['class'].map({'g': 1, 'h': 0})\n",
    "2. data['class'] = data['class'].replace({'g': 1, 'h': 0})\n",
    "3. from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    data['class'] = le.fit_transform(data['class'])\n",
    "4. data['class'] = np.where(data['class'] == 'g', 1, 0)\n",
    "```\n",
    "\n",
    "We are gonna use the most simplest of all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['class']= (df1['class'] == 'g').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097656a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdf6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in cols[:-1]:\n",
    "    \n",
    "    plt.hist(df1[df1['class']==1][i],color='blue',label = 'gamma',density=True)\n",
    "    plt.hist(df1[df1['class']==0][i],color='red',label = 'hydron',density=True)\n",
    "    plt.title(i)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel(i)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85cd964",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "#### Observations:\n",
    "\n",
    "1. The data is skewed in the Favour of Target = 'gamma'. Probably need to normalize that\n",
    "2. The scale of the Dataframe of every column is very skewed which can cause problem. So we need to scale that.\n",
    "3. From the Graphs above, few observations can be made.\n",
    "\n",
    "\n",
    "#### Next Steps:\n",
    "\n",
    "1. We will be dividing the Data set into Training, Validation and Testing.( 0-60, 60-80, 80-100)\n",
    "2. Seperate Input variables and Output Variables\n",
    "3. Scale all the columns using StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe92531",
   "metadata": {},
   "source": [
    "##### 1. Train, Validation, Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = np.split(df1.sample(frac = 1),[int(0.6*len(df1)),int(0.8*len(df1))])\n",
    "\n",
    "# Could have also been done thru train-test library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe17dfc",
   "metadata": {},
   "source": [
    "##### 2. Separating Input and Output Features and 3. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    X = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    data = np.hstack((X,np.reshape(y,(-1,1))))\n",
    "\n",
    "    return data, X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66707f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training Dataset\")\n",
    "print(len(train[train['class']==1]))\n",
    "print(len(train[train['class']==0]))\n",
    "\n",
    "print('Test Dataset')\n",
    "print(len(test[test['class']==1]))\n",
    "print(len(test[test['class']==0]))\n",
    "\n",
    "print('Valid Dataset')\n",
    "print(len(valid[valid['class']==1]))\n",
    "print(len(valid[valid['class']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7f09d",
   "metadata": {},
   "source": [
    "### Further Observations:\n",
    "1. As expected, there was a lot of imbalance in the training dataset itself. This could cause the model to biased.\n",
    "2. We would not been solving the imbalance for test and valid dataset but only for training dataset. This is done cause we have to see how our Model Performs on new data that could be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset_oversample(dataframe,oversample = False):\n",
    "    X = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler()\n",
    "        X,y = ros.fit_resample(X,y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    data = np.hstack((X,np.reshape(y,(-1,1))))\n",
    "\n",
    "    return data, X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae66821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, X_train, y_train = scale_dataset_oversample(train,oversample=True)\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17929128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y_train == 1))\n",
    "print(sum(y_train == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we mentioned, we would not be oversampling our Test and valid dataset.\n",
    "test, X_test, y_test = scale_dataset_oversample(test,oversample=False)\n",
    "valid, X_valid, y_valid = scale_dataset_oversample(valid,oversample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(X_train, \"X_train.pkl\")\n",
    "joblib.dump(X_test, \"X_test.pkl\")\n",
    "joblib.dump(X_valid, \"X_valid.pkl\")\n",
    "joblib.dump(y_train, \"y_train.pkl\")\n",
    "joblib.dump(y_test, \"y_test.pkl\")\n",
    "joblib.dump(y_valid, \"y_valid.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a3868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv CosmicClassifier)",
   "language": "python",
   "name": "cosmicclf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
